
<HTML> 
<HEAD> 
<meta content="text/html;charset=utf-8" http-equiv="Content-Type">
<link rel="stylesheet" href="nlp.css" type="text/css" media="all" /> 
<br>
<TITLE>NLP12 Assignment 1: Parts of Speech Tagging: Exploring Corpora, Error Analysis</TITLE> 
</HEAD> 
 
<BODY> 
 
<h1>Assignment 1</h1>

<b>Shimi Malka 066461641</b>
<br>
<b>Netali Alima 300712742</b>

<h2>Our Solutions: </h2>

<ol>
<li><a href="#data">Data Exploration</a>
   <ol>
   <li><a href="#crawl">Solutions for - Gathering and cleaning up data</a></li>
   <li><a href="#explore">Solutions for - Gathering basic statistics</a></li>
   <li><a href="#correlate">Solutions for - Is there a correlation between word size or frequency and ambiguity level?</a></li>
   </ol>
</li>
<li><a href="#unigram">Solutions for - Unigram and Affix Tagger</a></li>
<li><a href="#error">Fine-grained Error Analysis</a></li>
   <ol>
   <li><a href="#known">Solutions for - Known vs. Unknown Accuracy</a></li>
   <li><a href="#pertag">Solutions for - Per Tag Precision and Recall</a></li>
   <li><a href="#confusion">Solutions - for Confusion Matrix</a></li>
   <li><a href="#size">Solutions for - Sensitivity to the Size and Structure of the Training Set: Cross-Validation</a></li>
   <li><a href="#stratified">Solutions - for Stratified Samples</a></li>
   </ol>
</ol>

<a name="data"></a>
<h3>Data Exploration</h3>
<pre>
[our code]
</pre>

<a name="crawl"></a>
<h4>Gathering and Cleaning Up Data</h4>
<pre>
[our code]
</pre>


<a name="explore"></a>
<h4>Gathering Basic Statistics</h4>

<pre>
[our code]
</pre>

We expect this distribution to exhibit a "long tail" form.  Do you confirm this hypothesis?<br>
Answer:

<a name="correlate"></a>
<h4>Is there a correlation between word size or frequency and ambiguity level?</h4>
[graph]<br>
Describe what you observe.  Does the plot support a hypothesis about correlation? 
<br>
Answer: 

<a name="unigram">
<h3>Unigram and Affix Tagger</h3>
<h4>Unigram Tagger</h4>
<pre>
[our code]
</pre>

<h4>Affix Tagger</h4>
<pre>
[our code]
</pre>
Observations: 
<ol>
<li>does entropy filtering improve accuracy? <br>
Answer:
<li>how do you determine the range of values to test for the cutoff?<br>
Answer:
<li>is the accuracy value evolving in a predictable manner as the cutoff varies?<br>
Answer:
<li>describe the list of suffixes that are good tag predictors -- are you surprised by what you observe?<br>
Answer:
</ol>

<a name="error"></a>
<h3>Fine-Grained Accuracy and Error Analysis</h3>

<a name="known"></a>
<h4>Known vs. Unknown Accuracy</h4>

<pre>
[our code]
</pre>

<a name="pertag"></a>
<h4>Per Tag Precision and Recall</h4>

<pre>
[our code]
</pre>
To Check these Functions (precision and recall) we wrote 2 functions: <br>
1. checkTaggerPrecForTag(tagger, tag, testCorpus)<br>
2. checkTaggerRecallForTag(tagger, tag, testCorpus)<br>
And we run them twice with the default tagger (NN), for the first time with the tag "NN" and the second time with the tag "AT".<br>
Expected precision(for the default tag) = same value as evaluate function of TaggerI interface <br>
										  because TP = no. of words that the tagger tagged right<br>
                                           		  FP = no. of words that in the tagger tagged as 'chosen tag'<br>
                                           		  	   and the corpusTest tagged as different than the 'chosen tag'<br>
                                          => TP + FP = no. of words in the test<br>
Expected recall(for the default tag) = 1<br>
                                       because FN = 0 for the default tag (the tagger give each word the default tag)<br>
<br>
For any other tag the values should be 0 because TP = 0 for each tag<br>
<br>
We also can see that the FMeasure avg for the testCorpus is increasing when we improve the taggers (microEvaluate function results),
same as the original evaluate function of the TaggerI interface. <br>
<br>
Our Output:<br>
evaluate default nn  = 0.14594356261<br>
evaluate regExp(default nn)  = 0.330687830688<br>
evaluate affix(regExp(default nn))  = 0.4069664903<br>
evaluate unigram(affix(regExp(default nn)))  = 0.887125220459<br>
evaluate bigram(unigram(affix(regExp(default nn))))  = 0.900793650794<br>
<br>
micro-evaluate default nn  = 0.00299662766222<br>
micro-evaluate regExp(default nn)  = 0.0511631070415<br>
micro-evaluate affix(regExp(default nn))  = 0.19422104697<br>
micro-evaluate unigram(affix(regExp(default nn)))  = 0.765386700349<br>
micro-evaluate bigram(unigram(affix(regExp(default nn))))  = 0.802520278712<br>
<br>
default nn prec tag = AT  => 0<br>
default nn recall tag = AT  => 0.0<br>
<br>
default nn prec tag = NN  => 0.14594356261<br>
default nn recall tag = NN  => 1.0<br>
<br>
Which tags are most difficult in the simplified tagset? In the full tagset?<br>
Answer: Precision of Certain tag means How often the tagger is correct in the dataset when it predicts the tag.<br>
That's why we can say that if the precision of a certain tag is low then the tagger often wrong about this tag.<br>
In conclusion we can say that the tags with the lowest precision are the difficult tags.<br>
We Implemented few functions to deal with this issue:<br>
1.getTaggerAndTestSetInSimplifiedMode(taggerName):<br>
<li>Given a tagger name (DefaultTagger,RegExpTagger,AffixTagger,UnigramTagger,BigramTagger) - 
each tagger is the backoff of the next one,<br>
<li>the function defines the tagger, the train and test sets with the simplified tags set and returning the tagger and the test set.<br>
2.getDifficultTags(tagger, testCorpus, x, tagsSet):<br>
<li>Given a tagger, testSet, tagsSet and x (no. of difficult tags wanted),
returns list of difficult tags for the requested tagger.<br>
3.checkSimplifiedDifficultTags(taggerName, x):<br>
<li>Given a tagger Name and no. of requested difficult tags, returned the difficult tags.<br>
<li>Of course that this function defined the simplified tags set and calls getDifficultTags func and  getTaggerAndTestSetInSimplifiedMode func.<br>
4.checkFullDifficultTags(tagger, testCorpus, x):<br>
<li>Given a tagger and no. of requested difficult tags, returned the difficult tags.<br>
<li>Of course that this function defined the full tags set and calls getDifficultTags func.<br>

<pre>
Our code
</pre>
<a name="confusion"></a>
<h4>Confusion Matrix</h4>

<pre>
[our code]
</pre>
<p/>
Report the confusion matrix for the full tagset and simplified tagset of the Brown corpus for the last tagger
discussed in class.  Discuss the results: which pairs of tags are the most difficult to distinguish?
<p/>
Given your observation on the most likely confusions, propose a simple (engineering) method to improve the results
of your tagger.  Implement this improvement and report on error reduction.


<a name="size"></a>
<h4>Sensitivity to the Size and Structure of the Training Set: Cross-Validation</h4>

<pre>
[our code]
</pre>
Implement a method crossValidate(corpus, n) for trainable taggers.  Report the 10-fold cross-validation results for 
the last tagger discussed in class.  Discuss the results.

<a name="stratified"></a>
<h4>Stratified Samples</h4>
First Step: Finding the distribution of sentences by length
We Created a function that calculates the no. of sentences to each sentence's length and plot a graph that describe this distribution.<br>
In Class we said that the avg length of a sentence is 20, when we look on the graph we can see that 20 is the area where the no. of sentences is the highest.<br>
Also we can see that from length 20 to 40 the no. of sentences is decreasing rapidly and after 40 is the tail of the graph.<br>
As a result of this findings we decided:<br>
<li>The Short class will consist of sentences with length smaller than 20<br> 
<li>The Medium Class will consist of sentences with length 20-40<br>
<li>The Long Class will consist of sentences with length bigger than 80<br>
The Graph:<br>
<img src="DistLengthSenToNoSent.PNG" width="600" height="480" />
<pre>
#plot distribution of sentences by length
def getDistSentByLength():
    corpusSentences = []
    cats = brown.categories()
    for cat in cats:
        corpusSentences = corpusSentences + brown.sents(categories=cat)
    fd = nltk.FreqDist(len(sen) for sen in corpusSentences)
    
    length = sorted(fd.keys())
    noSent = [fd[n] for n in length]
    pylab.plot(length, noSent)
    pylab.title('Distribution of Sentences by Length')
    pylab.xlabel('Length')
    pylab.ylabel('Number of Sentences')
    pylab.grid(True)
    pylab.show()
</pre>
Discuss the results you observe.

<BR> 
<HR>
 <br>
</BODY> 
 
