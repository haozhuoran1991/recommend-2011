
<HTML> 
<HEAD> 
<meta content="text/html;charset=utf-8" http-equiv="Content-Type">
<link rel="stylesheet" href="nlp.css" type="text/css" media="all" /> 
<br>
<TITLE>NLP12 Project: Wikification</TITLE> 
</HEAD> 

<BODY> 
<h1>Project: Wikification</h1>
<b>Shimi Malka 066461641</b>
<br>
<b>Netali Alima 300712742</b>

<h2>Our Solutions: </h2>

This Project covers The process of building a model of wikipedia links and 
getting an article from the wikipedia clean the text from all wikipedia annotations
and recover the article according the model we learned.
<br>Finally calculate the Accuracy of this Decision process.
<br>

<h2>First Phase - Building the Train Data Set: </h2>
choosing X articles with more than N words and M links from the wikipedia:<br>
We started from a defined category and took all the links inside it and search for 
appropriate articles (that answer our conditions), It is a BFS scan.
<br>

<h2>Second Phase - Building the model & Training</h2>
The model is a cfd - (term,Link,no. of times the link appear for this term)<br>
For each article, go over all its links and fill the cfd accordingly.<br>
Notice that we are cleaning the term (the cleaning process will be explain later).
<br>

<h2>Third Phase - Building the Test data set:</h2>
Like building the train data set with the limitation of articles that appear in 
the train will not appear in the test.
<br>

<h2>Forth Phase - Decision process</h2>

<br>

<h2>Fifth Phase - Accuracy calculations</h2>

<br>

<h2>Third party Packages</h2>
<li>JWPL - Wikipedia API for parsing and working with wikipedia dump.<br>
<li>Jython - For running python scripts in java.<br>
<li>Meni's PhD thesis program - For segmentation and stemming.<br> 

<h2>Running Examples:</h2>
<br>
Running on 5000 articles in the train and 1000 in the test:<br>
<pre>

</pre>
<br>
Running on 500 articles in the train and 100 articles in the test:<br>
<pre>

</pre>  

</BODY>



