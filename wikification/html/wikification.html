
<HTML> 
<HEAD> 
<meta content="text/html;charset=utf-8" http-equiv="Content-Type">
<link rel="stylesheet" href="nlp.css" type="text/css" media="all" /> 
<br>
<TITLE>NLP12 Project: Wikification</TITLE> 
</HEAD> 

<BODY> 
<h1>Project: Wikification</h1>
<b>Shimi Malka 066461641</b>
<br>
<b>Netali Alima 300712742</b>

<h2>Our Solutions: </h2>

This Project covers The process of building a model of wikipedia links and 
getting an article from the wikipedia clean the text from all wikipedia annotations
and recover the article according the model we learned.
<br>Finally calculate the Accuracy of this Decision process.
<br>

<h2>First Phase - Building the Train Data Set: </h2>
choosing X articles with more than N words and M links from the wikipedia:<br>
We started from a defined category and took all the links inside it and search for 
appropriate articles (that answer our conditions), It is a BFS scan.
<br>

<h2>Second Phase - Building the model & Training</h2>
The model is a cfd - (term,Link,no. of times the link appear for this term)<br>
For each article, go over all its links and fill the cfd accordingly.<br>
Notice that we are cleaning the term (the cleaning process will be explain later).
<br>

<h2>Third Phase - Building the Test data set:</h2>
Like building the train data set with the limitation of articles that appear in 
the train will not appear in the test.
<br>

<h2>Forth Phase - Decision process</h2>
After we build the cfd (for each term we have the list of links 
associated to this term in all articles in the training set, 
and for each link and term we have the no. of times the link was associated to this term).<br>
We implemented the BaseLine decision process that means to take the most frequent link 
which was associated to the term wanted.
<br>

<h2>Fifth Phase - Accuracy calculations</h2>
After building the test data, for each article in the test data set we build a real map each
link we find in the page (map of term-link) then we clean the text of the article, 
and performing the decision process above for all terms in the page, the decision 
process gives us a map of term-link of our decisions.<br>
Then we compare the 2 maps and count the no. of hits in and sums it up for all articles.<br>
The accuracy is calculated and returned - Accuracy = no. of hits/no. of links in all articles.<br>
 
<br>

<h2>Third party Packages</h2>
<li>JWPL - Wikipedia API for parsing and working with wikipedia dump.<br>
<li>Jython - For running python scripts in java.<br>
<li>Meni's PhD thesis program - For segmentation and stemming.<br> 

<h2>Running Examples:</h2>
<br>
Running on 5000 articles in the train and 1000 in the test:<br>
<pre>

</pre>
<br>
Running on 500 articles in the train and 100 articles in the test:<br>
<pre>

</pre>  

</BODY>



