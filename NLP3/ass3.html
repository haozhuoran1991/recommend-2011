
<HTML> 
<HEAD> 
<meta content="text/html;charset=utf-8" http-equiv="Content-Type">
<link rel="stylesheet" href="nlp.css" type="text/css" media="all" /> 
<br>
<TITLE>NLP12 Assignment 3: NER, Parsing</TITLE> 
</HEAD> 

<BODY> 
<h1>Assignment 3</h1>
<b>Shimi Malka 066461641</b>
<br>
<b>Netali Alima 300712742</b>

<h2>Our Solutions: </h2>

This assignment covers 2 topics: 

<ol>
<li><a href="#svm">Machine Learning methods (SVM) for Named Entity Recognition</a></li>
<li><a href="#parsing">CFG Parsing</a></li>
</ol>
<hr/>

<a name="svm"></a>
<h2>Named Entity Recognition with SVM</h2>

<h3>Named Entity Recognition</h3>


<hr/>

<a name="parsing"></a>
<h2>Question 2: PCFG Parsing</h2>

<a name="q2.1"></a>
<h3>Question 2.1: Random PCFG Generation</h3>

<pre>
from nltk.grammar import Nonterminal ,toy_pcfg2
from nltk.probability import ConditionalFreqDist , FreqDist , DictionaryProbDist ,ProbDistI,  MLEProbDist
from nltk.tree import Tree
import numpy as np
import math

LhsProbDist = {}

def makeLhrProbDict(grammar):
    for nonTr in grammar.productions():
        nonTr_productions = grammar.productions(nonTr.lhs())
        dict = {}
        for pr in nonTr_productions:
            dict[pr.rhs()] = pr.prob()
        
        probDist = DictionaryProbDist(dict)
        LhsProbDist[nonTr.lhs()] = probDist

# return a tree sampled from the language described by the grammar
def pcfg_generate(grammar):
    start = grammar.start()
    return generate_one(grammar, [start])

def generate_one(grammar, items):
    if len(items) == 1 :
        if isinstance(items[0], Nonterminal):
            rhs = LhsProbDist[items[0]].generate()
            return Tree(str(items[0]), generate_one(grammar, rhs))
        else:
            return items
    else:
        l = [] 
        for r in items:
            l.append(generate_one(grammar, [r]))
        return l

# - Generate 1,000 random sentences using nltk.grammar.toy_pcfg2
# - Compute the frequency distribution of each non-terminal and pre-terminal in the generated corpus.
# - For each distribution, compute the KL-divergence between the MLE estimation of the probability
#      distribution constructed on your test corpus and toy_pcfg2.  
def validate_pcfg_generate(grammar):
    productions = []
    cfd = ConditionalFreqDist()
    
    for i in np.arange(1000):
        tree = pcfg_generate(grammar)
        productions += tree.productions()    

    for p in productions:
        cfd[p.lhs()].inc(p.rhs())
        
    for c in cfd.conditions():
        p = MLEProbDist(cfd[c])
        q = LhsProbDist[c]
        div = KL_Divergence(p,q)
        print "KL_Divergence for %s = %f" %(c , div)
    
# calc KL div between p ang q safely
def KL_Divergence(p,q):
    eps = 0.0001
    SP = set(p.samples()) 
    SQ = set(q.samples())
    if (len(SP) == 0) | (len(SQ) == 0):
        return -1
    SU = SP | SQ
    pc = eps*len(SU - SP)/len(SP) 
    qc = eps*len(SU - SQ)/len(SQ)
    Ptag = []
    Qtag = []
    for x in SU:
        if x in SP:
            Ptag.append(p.prob(x)-pc)
        else: Ptag.append(eps)
        if x in SQ:
            Qtag.append(q.prob(x)-qc)
        else: Qtag.append(eps)
    div = 0
    for pi , qi in zip(Ptag,Qtag):
        d = pi / qi
        if (d != 0):
            div += pi * math.log(d) 
    return div
    

def main():
    grammar = toy_pcfg2
    LhsProbDist = makeLhrProbDict(grammar)
    validate_pcfg_generate(grammar)
    
if __name__ == '__main__':
    main() 
</pre>

<h4>Validation</h4>
KL_Divergence for Det = 0.001212<br>
KL_Divergence for N = 0.000504<br>
KL_Divergence for NP = 2.026744<br>
KL_Divergence for P = 0.000050<br>
KL_Divergence for PP = 0.000000<br>
KL_Divergence for S = 0.000000<br>
KL_Divergence for V = 0.004286<br>
KL_Divergence for VP = 2.982441<br>

Explain:<br>
We think the pre-terminals get small divergence cause there are many terminal to choose from,<br>
and the interiors non terminal with divergence 0 cause there are less options to choose from.<br> 

<a name="q2.2"></a> 
<h3>Question 2.2: Learn a PCFG from a Treebank</h3> 

<pre> 
import nltk
import nltk.grammar as gram
from nltk.probability import DictionaryProbDist , FreqDist
from nltk.grammar import WeightedGrammar , WeightedProduction , Nonterminal
from nltk.corpus import LazyCorpusLoader, BracketParseCorpusReader , simplify_wsj_tag
import matplotlib.pyplot as plt
import numpy as np

# filter NONE noneterminal from the tree
def filter_NONE(tree):
    if isinstance(tree, str):
        return tree
    if tree.node =='-NONE-':
        return None
    f_childrens = []
    for child in tree[0:]:
        c = filter_NONE(child)
        if c != None :
            f_childrens.append(c)
    if len(f_childrens) == 0: return None
    return nltk.Tree(tree.node,f_childrens)

# create Weighted Grammar for given productions
def createWG(productions):
    pcount = {} 
    lcount = {}
    for prod in productions:
        lcount[prod.lhs()] = lcount.get(prod.lhs(), 0) + 1
        pcount[prod] = pcount.get(prod, 0) + 1
    
    prods = [WeightedProduction(p.lhs(), p.rhs(), 
             prob=float(pcount[p]) / lcount[p.lhs()]) for 
             p in pcount]
    learned_pcfg_cnf = WeightedGrammar(Nonterminal('S'), prods)
    return learned_pcfg_cnf


# - treebank is the nltk.corpus.treebank lazy corpus reader
# - n indicates the number of trees to read
# - return an nltk.WeigthedGrammar
def pcfg_learn(treebank, n):
    productions = []
    treebank_interior_nodes = 0;
    
    for item in treebank.items[:n]:
        for tree in treebank.parsed_sents(item):
            treebank_interior_nodes += len(tree.productions()) + len(tree.leaves())
            tree = filter_NONE(tree)
            if tree!= None:
                productions += tree.productions()
            
    learned_pcfg = createWG( productions)

    plot_dist_productions_by_frequency(productions)
    print 'How many productions are learned from the trees? %d ' % len(learned_pcfg.productions())
    print 'How many interior nodes were in the treebank?    %d ' % treebank_interior_nodes
    return  learned_pcfg

#-- treebank is the nltk.corpus.treebank lazy corpus reader (simplified tags)
#-- n indicates the number of trees to read
#-- return an nltk.WeigthedGrammar in CNF
def pcfg_cnf_learn(treebank, n):
    productions = []
    treebank_interior_nodes = 0
    cnf_interior_nodes = 0
    
    for item in treebank.items[:n]:
        for tree in treebank.parsed_sents(item):
            treebank_interior_nodes += len(tree.productions()) + len(tree.leaves())
            tree = filter_NONE(tree)
            if tree!= None:
                tree.chomsky_normal_form(horzMarkov = 2)
                cnf_interior_nodes += len(tree.productions()) + len(tree.leaves())
                productions += tree.productions()
            
    learned_pcfg_cnf = createWG(productions)
    
    print 'How many productions are learned from the CNF trees?   %d ' % len(learned_pcfg_cnf.productions())
    print 'How many interior nodes were in the original treebank? %d ' %  treebank_interior_nodes
    print 'How many interior nodes were in the CNF treebank?      %d ' % cnf_interior_nodes
    return learned_pcfg_cnf 

def plot_dist_productions_by_frequency(productions):
    f= FreqDist(productions)
    fdd = FreqDist(f.values())
    x = []
    y = []
    for k in fdd.keys():
        x.append(k)
        y.append(fdd[k])
    plt.plot(x,y,lw=2,color= 'b')
    plt.title('Productions by frequency' )
    plt.xlabel('frequency')
    plt.ylabel('number of rules with frequency')
    plt.show()

# determines whether a tree can be parsed by a grammar
# tests that a given tree can be produced by a grammar
def cover_tree(grammar, tree):
    for pt in tree.productions():
        found = False
        for pg in grammar.productions():
            if (pt.rhs()== pg.rhs()) & (pt.lhs()==pg.lhs()):
                found = True 
                break
        if not found :
            return False
    return True

# keep only the F most frequent rules out of the N rules in the PCFG
# return the number of trees "missed" by the new pcfg
def count_misses(pcfg,treebank,n):
    misses = 0
    for item in treebank.items[:n]:
        for tree in treebank.parsed_sents(item):
            tree = filter_NONE(tree)
            if not cover_tree(pcfg, tree):
                misses +=1
    return misses


# Assume we "cut" the tail of the learned PCFG, that is we remove the least frequent rules,
# so that we keep only the F most frequent rules out of the N rules in the PCFG
# Draw a plot that indicates the number of trees "missed" 
# as the number of rules is reduced (sample every 10% of the size of the grammar).   
def plot_misses(pcfg,treebank,n):
    productions = []
    for item in treebank.items[:n]:
        for tree in treebank.parsed_sents(item):
            tree = filter_NONE(tree)
            if tree!= None:
                productions += tree.productions()
    fk= FreqDist(productions).keys()
    
    x = []
    y = []
    for reduced in np.arange(10):
        F = int(len(fk)*(reduced*0.1))
        x.append(F)
        prodsTake = list(productions)
        for k in fk[len(fk)-F:]:
            prodsTake.remove(k)
        if len(prodsTake)==0:
            y.append(len(prodsTake))
            continue
        cutPcfg = createWG(prodsTake)
        y.append(count_misses(cutPcfg,treebank,n))
        
    plt.plot(x,y,lw=1.5,color= 'b')
    plt.title('cut the tail of the learned PCFG' )
    plt.xlabel('F cuted from pcfg')
    plt.ylabel('misses')
    plt.show()     
    
     
def main():    
    n = 20
    treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, 
                                r'wsj_.*\.mrg', tag_mapping_function=simplify_wsj_tag)
    print "--PCFG--" 
    learned_pcfg = pcfg_learn(treebank, n)
    plot_misses(learned_pcfg,treebank,n) 
    print "\n--CNF PCFG--" 
    learned_pcfg_cnf = pcfg_cnf_learn(treebank, n) 
    
if __name__ == '__main__':
    main() 
</pre> 

<h4>Data Exploration and Validation</h4>
Output:<br>
--PCFG--<br>
How many productions are learned from the trees? 2982<br> 
How many interior nodes were in the treebank?    16543<br>
<img src="q2_2a.PNG" width="600" height="480" />
<img src="q2_2b.PNG" width="600" height="480" />

<h4>CNF PCFG</h4>
Output:<br>
--PCFG--<br>
How many productions are learned from the trees? 2982<br> 
How many interior nodes were in the treebank?    16543 <br>

--CNF PCFG--
How many productions are learned from the CNF trees?   3535 
How many interior nodes were in the original treebank? 16543 
How many interior nodes were in the CNF treebank?      17373

<a name="q2.3"></a> 
<h3>Question 2.3: Test CFG Independence Assumptions</h3> 

<a name="q2.4"></a> 
<h3>Question 2.4: Learn a bigram Language Model</h3> 

<a name="q2.5"></a> 
<h3>Question 2.5: Validation of PCFG Generated text using an n-gram Model</h3> 

<BR> 
<HR>
 <br>
</BODY>



